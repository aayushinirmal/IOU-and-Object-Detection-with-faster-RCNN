{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSfnuObtYYMH"
   },
   "source": [
    "\n",
    "## Description\n",
    "---\n",
    "Detection is a fundamental task in CV. This homework is on faster-RCNN, one of the most famous two-stage detection models. You will first implement some basic functions of detection, including generating anchor boxes of different scales and ratios and calculating iou. Then you will train and test on the [UAVDT dataset](https://sites.google.com/site/daviddo0323/projects/uavdt) with faster-RCNN ([faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch)) to see how a detection model really works.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "id": "VCoZ4NhzAfOd",
    "outputId": "42fa667b-24d0-4d9d-d756-527944fa5f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu100/torch_stable.html\n",
      "Collecting torch==1.0.0\n",
      "  Downloading https://download.pytorch.org/whl/cu100/torch-1.0.0-cp37-cp37m-win_amd64.whl (724.7MB)\n",
      "Collecting torchvision==0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torchvision==0.2.1) (5.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torchvision==0.2.1) (1.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from torchvision==0.2.1) (1.15.1)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.0.0 torchvision-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 21.2.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Pillow==6.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/75/1edde839a0f50fdfbf18226260d43b216ccd11ca69a1fc6ebdc664de64ae/Pillow-6.2.2-cp37-cp37m-win_amd64.whl (2.0MB)\n",
      "Installing collected packages: Pillow\n",
      "  Found existing installation: Pillow 5.2.0\n",
      "    Uninstalling Pillow-5.2.0:\n",
      "      Successfully uninstalled Pillow-5.2.0\n",
      "Successfully installed Pillow-6.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 21.2.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/58/f0/d00c0e01e077da883f030af3ff5ce653a0e9e4786f83faa89a6e18c98612/scipy-1.2.1-cp37-cp37m-win_amd64.whl (30.0MB)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scipy==1.2.1) (1.15.1)\n",
      "Installing collected packages: scipy\n",
      "  Found existing installation: scipy 1.1.0\n",
      "    Uninstalling scipy-1.1.0:\n",
      "      Successfully uninstalled scipy-1.1.0\n",
      "Successfully installed scipy-1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 21.2.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# remember to RESTART runtime when you see prompt asking to do so\n",
    "!pip install torch==1.0.0 torchvision==0.2.1 -f https://download.pytorch.org/whl/cu100/torch_stable.html\n",
    "!pip install Pillow==6.2.2\n",
    "!pip install scipy==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g01x8yQKBtGM",
    "outputId": "67ccfc37-4146-4c10-85db-02009144bb53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ThW7hYKSiMrh"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7cfacd8d75af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YH6RivrIGFgA",
    "outputId": "b34fece3-cafa-469d-bd58-75942d79b090"
   },
   "outputs": [],
   "source": [
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7T72O-1ks-a",
    "outputId": "0f196edc-691d-4933-e38b-b8c00ae6e1f8"
   },
   "outputs": [],
   "source": [
    "# Mount your google drive where you've saved your assignment folder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2s75J7ApdTg",
    "outputId": "c785724e-0c48-49f8-d1e3-fe98daa6a4a6"
   },
   "outputs": [],
   "source": [
    "cd \"/content/gdrive/MyDrive/nirmal_aayushi_113504530_hw5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrJQLGBZUkIi",
    "outputId": "b2c9dacb-10d3-4e8d-d590-d5570f1b7e8a"
   },
   "outputs": [],
   "source": [
    "!tar -xvf data_uav.tar.gz -C faster-rcnn.pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKYq7E9l5cCh",
    "outputId": "f738d366-c382-4041-c2c1-d7ef26d0e05d"
   },
   "outputs": [],
   "source": [
    "!wget -P faster-rcnn.pytorch/data/pretrained_model https://filebox.ece.vt.edu/~jw2yang/faster-rcnn/pretrained-base-models/vgg16_caffe.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7BXH_eL5iQo",
    "outputId": "8709d168-5dc7-422c-c114-2130e4ac917a"
   },
   "outputs": [],
   "source": [
    "cd faster-rcnn.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPBBVxo76FZj",
    "outputId": "fc8b4803-7c7c-49da-d20d-d81fc33bce32"
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hchFpOrO6PHT",
    "outputId": "c92c49d5-2171-42a2-b317-c43074c65257"
   },
   "outputs": [],
   "source": [
    "cd lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bsxp--H8NRI5",
    "outputId": "180e93ee-60da-468c-a3f7-36d580e5132c"
   },
   "outputs": [],
   "source": [
    "!python setup.py build develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LtyCvwSOthm",
    "outputId": "5cfcafee-de50-43c7-b107-aa7026ee5bc0"
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "y68s6GN-3Wfi",
    "outputId": "7b9fde50-ae46-47ec-db83-62b90d82e5e4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get ground-truth annotation\n",
    "\n",
    "def load_pascal_annotation(data_path, index, num_classes):\n",
    "  \"\"\"\n",
    "  Load image and bounding boxes info from XML file in the PASCAL VOC\n",
    "  format.\n",
    "  \"\"\"\n",
    "  filename = os.path.join(data_path, 'Annotations', index + '.xml')\n",
    "  tree = ET.parse(filename)\n",
    "  objs = tree.findall('object')\n",
    "\n",
    "  # Exclude the samples labeled as difficult\n",
    "  non_diff_objs = [\n",
    "      obj for obj in objs if int(obj.find('difficult').text) == 0]\n",
    "  if len(non_diff_objs) != len(objs):\n",
    "      print('Removed {} difficult objects'.format(\n",
    "          len(objs) - len(non_diff_objs)))\n",
    "  objs = non_diff_objs\n",
    "  num_objs = len(objs)\n",
    "\n",
    "  boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n",
    "  gt_classes = [i for i in range(num_objs)]\n",
    "\n",
    "  # Load object bounding boxes into a data frame.\n",
    "  for ix, obj in enumerate(objs):\n",
    "      bbox = obj.find('bndbox')\n",
    "      # Make pixel indexes 0-based\n",
    "      x1 = float(bbox.find('xmin').text) - 1\n",
    "      y1 = float(bbox.find('ymin').text) - 1\n",
    "      x2 = float(bbox.find('xmax').text) - 1\n",
    "      y2 = float(bbox.find('ymax').text) - 1\n",
    "      cls = obj.find('name').text.lower().strip()\n",
    "      boxes[ix, :] = [x1, y1, x2, y2]\n",
    "      gt_classes[ix] = cls\n",
    "\n",
    "  return {'boxes' : boxes,\n",
    "          'gt_classes': gt_classes}\n",
    "\n",
    "# plot bounding box on image\n",
    "def vis_detections(im, bbox, color=(0, 204, 0), clip=None, width=2):\n",
    "  im_ = np.copy(im)\n",
    "  for i in range(len(bbox)):\n",
    "    cv2.rectangle(im_, (int(bbox[i][0]),int(bbox[i][1])), (int(bbox[i][2]),int(bbox[i][3])), color, width)\n",
    "\n",
    "  # Be sure to convert the color space of the image from\n",
    "  # BGR (Opencv) to RGB (Matplotlib) before you show a \n",
    "  # color image read from OpenCV\n",
    "  plt.figure(figsize=(20, 10));\n",
    "  plt.subplot(1, 2, 1);\n",
    "  if clip is not None:\n",
    "    plt.imshow(cv2.cvtColor(im_[clip[0]:clip[2],clip[1]:clip[3],:], cv2.COLOR_BGR2RGB));\n",
    "  else:\n",
    "    plt.imshow(cv2.cvtColor(im_, cv2.COLOR_BGR2RGB));\n",
    "  plt.title('train image (bounding box)');\n",
    "  plt.axis(\"off\");\n",
    "  \n",
    "  return im_\n",
    "\n",
    "# read the image for local directory (same with this .ipynb) \n",
    "img = cv2.imread('images/img0.jpg')\n",
    "\n",
    "data_path = 'data/VOCdevkit2007/UAV2017/'\n",
    "index = 'M0101_1'\n",
    "gt = load_pascal_annotation(data_path, index, 2)\n",
    "\n",
    "print(\"{} ground-truth bounding boxes in all\".format(len(gt['boxes'])))\n",
    "for _i in range(len(gt['boxes'])):\n",
    "  print('bbox{}: {} class: {}'.format(_i, gt['boxes'][_i],gt['gt_classes'][_i]))\n",
    "\n",
    "img_gt = vis_detections(img, gt['boxes'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbF88rI5ww_j"
   },
   "source": [
    "## Anchor boxes, Scales and Aspect Ratios. Intersection of Union.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "Eb1VSN_uUxRR",
    "outputId": "c31542c0-202f-4519-d329-fa8a695f2b50"
   },
   "outputs": [],
   "source": [
    "# generate anchor boxes of different scales and ratios\n",
    "def generate_anchors(center, scales, ratios, base_size=4):  \n",
    "    # base_size is your anchor box size when scale=1 and ratio=1\n",
    "    ##########--WRITE YOUR CODE HERE--########## \n",
    "\n",
    "    num_sizes, num_ratios = len(scales), len(ratios)\n",
    "    boxes_ppx = num_ratios * num_sizes\n",
    "    scale_tensor = np.array(scales)\n",
    "    ratio_tensor = np.array(ratios)\n",
    "    center_w, center_h = center[0], center[1]\n",
    "    shift_x, shift_y = np.meshgrid(center_w, center_h)\n",
    "    shift_x, shift_y = shift_x.reshape(-1), shift_y.reshape(-1)\n",
    "    w = np.concatenate((scale_tensor * np.sqrt(ratio_tensor[0]), scale_tensor * np.sqrt(ratio_tensor[1]), scale_tensor * np.sqrt(ratio_tensor[2])))\n",
    "    h = np.concatenate((scale_tensor / np.sqrt(ratio_tensor[0]), scale_tensor / np.sqrt(ratio_tensor[1]), scale_tensor / np.sqrt(ratio_tensor[2])))\n",
    "    anchor_manipulations = np.stack((-w, -h, w, h)).T\n",
    "    out_grid = np.stack([shift_x, shift_y, shift_x, shift_y], axis=1).repeat(boxes_ppx, axis=0)\n",
    "    anchors = out_grid + anchor_manipulations\n",
    "    # print(anchors.shape)\n",
    "\n",
    "\n",
    "   ##########-------END OF CODE-------########## \n",
    "    return anchors\n",
    "\n",
    "center = [190, 170]\n",
    "anchor_scales = [8, 16, 32]\n",
    "anchor_ratios = [0.5, 1, 2]\n",
    "bbox = generate_anchors(center, anchor_scales, anchor_ratios)\n",
    "# print(bbox[1,:])\n",
    "# crop image for better view\n",
    "img_gt_a = vis_detections(img_gt, bbox, (0,0,255), (0,0,300,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "a6iGnl8yuOyu",
    "outputId": "70d8f527-3233-4793-f260-c4b45302bc5e"
   },
   "outputs": [],
   "source": [
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    \"\"\"Calculate the Intersection of Unions (IoUs) between bounding boxes.\n",
    "\n",
    "    IoU is calculated as a ratio of area of the intersection\n",
    "    and area of the union.\n",
    "\n",
    "    both inputs and output should be python list\n",
    "    \"\"\"\n",
    "    ##########--WRITE YOUR CODE HERE--##########\n",
    "    bbox_a = torch.tensor(bbox_a)\n",
    "    bbox_b = torch.tensor(bbox_b)\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n",
    "                              (boxes[:, 3] - boxes[:, 1]))\n",
    "    area1 = box_area(bbox_a)\n",
    "    area2 = box_area(bbox_b)\n",
    "    bbox_b = bbox_b.long()\n",
    "    lt = torch.max(bbox_a[:, None, :2], bbox_b[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(bbox_a[:, None, 2:], bbox_b[:, 2:])  # [N,M,2]\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "    inter = inter.double()\n",
    "    area1 = area1.double()\n",
    "    area2 = area2.double()\n",
    "    unioun = area1[:, None] + area2 - inter\n",
    "    iou = inter / unioun\n",
    "    ##########-------END OF CODE-------########## \n",
    "    return iou\n",
    "\n",
    "bbox_gt = [[141,147,247,192]]\n",
    "# print(bbox_gt.shape)\n",
    "# bbox_gt = torch.tensor(bbox_gt)\n",
    "iou = bbox_iou(bbox_gt, bbox)\n",
    "bbox_filter = []\n",
    "iou_max = 0.0\n",
    "bbox_max = None\n",
    "for bbox_, iou_ in zip(bbox, iou[0]):\n",
    "  # get bbox of maximum iou\n",
    "  if iou_ > iou_max:\n",
    "    iou_max = iou_\n",
    "    bbox_max = bbox_\n",
    "  # only keep bbox of high iou as postive and low iou as negative\n",
    "  if iou_>=0.25 and iou_<=0.7:\n",
    "    continue\n",
    "  print(\"gt:{}  bbox:{}  iou：{:.4f}\".format(bbox_gt[0], bbox_, iou_))\n",
    "  bbox_filter.append(bbox_)\n",
    "\n",
    "img_gt_b = vis_detections(img_gt, bbox_filter, (0,0,255), (0,0,300,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "aXbxOcJM7xWD",
    "outputId": "be9fdbc6-5103-424e-f2b4-e4c44776bb1c"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "print(\"gt:{}  bbox_max:{}  iou_max：{:.4f}\".format(bbox_gt[0], bbox_max, iou_max))\n",
    "\n",
    "# for i in range(len(bbox_max)):\n",
    "#   bbox_max[i] = np.int8(bbox_max[i])\n",
    "bbox_max = np.array(bbox_max)\n",
    "bbox_max = np.int8(bbox_max)\n",
    "print(bbox_max.dtype)\n",
    "def roi_pooling(roi, pooling_shape):\n",
    "  ##########--WRITE YOUR CODE HERE--##########\n",
    "  # check the shape of roi first and pay attention to the channel dimension\n",
    "  r,c, channels = roi.shape\n",
    "  roip = np.zeros((pooling_shape[0], pooling_shape[1], channels))\n",
    "  k = 0\n",
    "  for i in range(0, pooling_shape[0]):\n",
    "    l = 0\n",
    "    x_inc = (int)((i+1)*r/pooling_shape[0])\n",
    "    for j in range(0, pooling_shape[1]):\n",
    "      y_inc = (int)((j+1)*c/pooling_shape[1])\n",
    "      roip[i][j][0] = np.max(roi[k:x_inc, l:y_inc, 0:1])\n",
    "      roip[i][j][1] = np.max(roi[k:x_inc, l:y_inc, 1:2])\n",
    "      roip[i][j][2] = np.max(roi[k:x_inc, l:y_inc, 2:3])\n",
    "      l = y_inc\n",
    "    k = x_inc\n",
    "  roip = roip.astype(np.uint8)\n",
    "\n",
    "\n",
    "  ##########-------END OF CODE-------##########\n",
    "  return roip\n",
    "\n",
    "# pooling on anchor box of maximum iou\n",
    "roi = img[bbox_max[1]:bbox_max[3],bbox_max[0]:bbox_max[2],:]\n",
    "roip = roi_pooling(roi, (25,25)) \n",
    "print(roip.shape)\n",
    "\n",
    "# pooling on whole image\n",
    "imgp = roi_pooling(img, (25,25)) \n",
    "print(imgp.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 8));\n",
    "plt.subplot(1, 2, 1);\n",
    "plt.imshow(cv2.cvtColor(roip, cv2.COLOR_BGR2RGB));\n",
    "plt.subplot(1, 2, 2);\n",
    "plt.imshow(cv2.cvtColor(imgp, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx8pbgK9zjaM"
   },
   "source": [
    "## Faster-RCNN\n",
    "\n",
    " Unmanned Aerial Vehicles (UAVs) are used to fuel numerous important applications in computer vision. This UAVDT dataset contains several videos of traffic. Each video is provided as a sequence of JPEG images. It only has one class of object, i.e., car. The subset contains only two videos for training and one video for testing, just to reduce time.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCd0C_REtYKk",
    "outputId": "e04ebe13-d8f9-457a-b35e-4e8911ba6359"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Pytorch multi-GPU Faster R-CNN\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n",
    "# Modified by Tao for HW5\n",
    "# --------------------------------------------------------\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "from roi_data_layer.roidb import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n",
    "      adjust_learning_rate, save_checkpoint, clip_gradient\n",
    "\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.faster_rcnn.resnet import resnet\n",
    "\n",
    "def parse_args(args):\n",
    "  \"\"\"\n",
    "  Parse input arguments\n",
    "  \"\"\"\n",
    "  parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n",
    "  parser.add_argument('--dataset', dest='dataset',\n",
    "                      help='training dataset',\n",
    "                      default='pascal_voc', type=str)\n",
    "  parser.add_argument('--net', dest='net',\n",
    "                    help='vgg16, res101',\n",
    "                    default='vgg16', type=str)\n",
    "  parser.add_argument('--start_epoch', dest='start_epoch',\n",
    "                      help='starting epoch',\n",
    "                      default=1, type=int)\n",
    "  parser.add_argument('--epochs', dest='max_epochs',\n",
    "                      help='number of epochs to train',\n",
    "                      default=20, type=int)\n",
    "  parser.add_argument('--disp_interval', dest='disp_interval',\n",
    "                      help='number of iterations to display',\n",
    "                      default=100, type=int)\n",
    "  parser.add_argument('--checkpoint_interval', dest='checkpoint_interval',\n",
    "                      help='number of iterations to display',\n",
    "                      default=10000, type=int)\n",
    "\n",
    "  parser.add_argument('--save_dir', dest='save_dir',\n",
    "                      help='directory to save models', default=\"models\",\n",
    "                      type=str)\n",
    "  parser.add_argument('--nw', dest='num_workers',\n",
    "                      help='number of workers to load data',\n",
    "                      default=0, type=int)\n",
    "  parser.add_argument('--cuda', dest='cuda',\n",
    "                      help='whether use CUDA',\n",
    "                      action='store_true')\n",
    "  parser.add_argument('--ls', dest='large_scale',\n",
    "                      help='whether use large imag scale',\n",
    "                      action='store_true')                      \n",
    "  parser.add_argument('--mGPUs', dest='mGPUs',\n",
    "                      help='whether use multiple GPUs',\n",
    "                      action='store_true')\n",
    "  parser.add_argument('--bs', dest='batch_size',\n",
    "                      help='batch_size',\n",
    "                      default=1, type=int)\n",
    "  parser.add_argument('--cag', dest='class_agnostic',\n",
    "                      help='whether to perform class_agnostic bbox regression',\n",
    "                      action='store_true')\n",
    "\n",
    "# config optimization\n",
    "  parser.add_argument('--o', dest='optimizer',\n",
    "                      help='training optimizer',\n",
    "                      default=\"sgd\", type=str)\n",
    "  parser.add_argument('--lr', dest='lr',\n",
    "                      help='starting learning rate',\n",
    "                      default=0.001, type=float)\n",
    "  parser.add_argument('--lr_decay_step', dest='lr_decay_step',\n",
    "                      help='step to do learning rate decay, unit is epoch',\n",
    "                      default=5, type=int)\n",
    "  parser.add_argument('--lr_decay_gamma', dest='lr_decay_gamma',\n",
    "                      help='learning rate decay ratio',\n",
    "                      default=0.1, type=float)\n",
    "\n",
    "# set training session\n",
    "  parser.add_argument('--s', dest='session',\n",
    "                      help='training session',\n",
    "                      default=1, type=int)\n",
    "\n",
    "# resume trained model\n",
    "  parser.add_argument('--r', dest='resume',\n",
    "                      help='resume checkpoint or not',\n",
    "                      default=False, type=bool)\n",
    "  parser.add_argument('--checksession', dest='checksession',\n",
    "                      help='checksession to load model',\n",
    "                      default=1, type=int)\n",
    "  parser.add_argument('--checkepoch', dest='checkepoch',\n",
    "                      help='checkepoch to load model',\n",
    "                      default=1, type=int)\n",
    "  parser.add_argument('--checkpoint', dest='checkpoint',\n",
    "                      help='checkpoint to load model',\n",
    "                      default=0, type=int)\n",
    "# log and display\n",
    "  parser.add_argument('--use_tfb', dest='use_tfboard',\n",
    "                      help='whether use tensorboard',\n",
    "                      action='store_true')\n",
    "\n",
    "  args = parser.parse_args(args)\n",
    "\n",
    "  return args\n",
    "\n",
    "\n",
    "class sampler(Sampler):\n",
    "  def __init__(self, train_size, batch_size):\n",
    "    self.num_data = train_size\n",
    "    self.num_per_batch = int(train_size / batch_size)\n",
    "    self.batch_size = batch_size\n",
    "    self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n",
    "    self.leftover_flag = False\n",
    "    if train_size % batch_size:\n",
    "      self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n",
    "      self.leftover_flag = True\n",
    "\n",
    "  def __iter__(self):\n",
    "    rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n",
    "    self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n",
    "\n",
    "    self.rand_num_view = self.rand_num.view(-1)\n",
    "\n",
    "    if self.leftover_flag:\n",
    "      self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n",
    "\n",
    "    return iter(self.rand_num_view)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.num_data\n",
    "\n",
    "\n",
    "# add your args here\n",
    "args = parse_args(['--dataset','uav','--net','vgg16','--bs','8','--lr','1e-2','--lr_decay_step','4','--epochs','2','--cuda'])\n",
    "\n",
    "print('Called with args:')\n",
    "print(args)\n",
    "\n",
    "args.imdb_name = \"uav_2017_trainval\"\n",
    "args.imdbval_name = \"uav_2017_test\"\n",
    "args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '20']\n",
    "\n",
    "\n",
    "args.cfg_file = \"cfgs/{}_ls.yml\".format(args.net) if args.large_scale else \"cfgs/{}.yml\".format(args.net)\n",
    "\n",
    "if args.cfg_file is not None:\n",
    "  cfg_from_file(args.cfg_file)\n",
    "if args.set_cfgs is not None:\n",
    "  cfg_from_list(args.set_cfgs)\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "np.random.seed(cfg.RNG_SEED)\n",
    "\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "if torch.cuda.is_available() and not args.cuda:\n",
    "  print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "# train set\n",
    "# -- Note: Use validation set and disable the flipped to enable faster loading.\n",
    "cfg.TRAIN.USE_FLIPPED = False\n",
    "cfg.USE_GPU_NMS = args.cuda\n",
    "imdb, roidb, ratio_list, ratio_index = combined_roidb(args.imdb_name)\n",
    "train_size = len(roidb)\n",
    "\n",
    "print('{:d} roidb entries'.format(len(roidb)))\n",
    "\n",
    "output_dir = args.save_dir + \"/\" + args.net + \"/\" + args.dataset\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)\n",
    "\n",
    "sampler_batch = sampler(train_size, args.batch_size)\n",
    "\n",
    "dataset = roibatchLoader(roidb, ratio_list, ratio_index, args.batch_size, \\\n",
    "                          imdb.num_classes, training=True)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size,\n",
    "                          sampler=sampler_batch, num_workers=args.num_workers)\n",
    "\n",
    "# initilize the tensor holder here.\n",
    "im_data = torch.FloatTensor(1)\n",
    "im_info = torch.FloatTensor(1)\n",
    "num_boxes = torch.LongTensor(1)\n",
    "gt_boxes = torch.FloatTensor(1)\n",
    "\n",
    "# ship to cuda\n",
    "if args.cuda:\n",
    "  im_data = im_data.cuda()\n",
    "  im_info = im_info.cuda()\n",
    "  num_boxes = num_boxes.cuda()\n",
    "  gt_boxes = gt_boxes.cuda()\n",
    "\n",
    "# make variable\n",
    "im_data = Variable(im_data)\n",
    "im_info = Variable(im_info)\n",
    "num_boxes = Variable(num_boxes)\n",
    "gt_boxes = Variable(gt_boxes)\n",
    "\n",
    "if args.cuda:\n",
    "  cfg.CUDA = True\n",
    "\n",
    "# initilize the network here.\n",
    "if args.net == 'vgg16':\n",
    "  fasterRCNN = vgg16(imdb.classes, pretrained=True, class_agnostic=args.class_agnostic)\n",
    "elif args.net == 'res101':\n",
    "  fasterRCNN = resnet(imdb.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)\n",
    "elif args.net == 'res50':\n",
    "  fasterRCNN = resnet(imdb.classes, 50, pretrained=True, class_agnostic=args.class_agnostic)\n",
    "elif args.net == 'res152':\n",
    "  fasterRCNN = resnet(imdb.classes, 152, pretrained=True, class_agnostic=args.class_agnostic)\n",
    "else:\n",
    "  print(\"network is not defined\")\n",
    "  pdb.set_trace()\n",
    "\n",
    "fasterRCNN.create_architecture()\n",
    "\n",
    "lr = cfg.TRAIN.LEARNING_RATE\n",
    "lr = args.lr\n",
    "\n",
    "params = []\n",
    "for key, value in dict(fasterRCNN.named_parameters()).items():\n",
    "  if value.requires_grad:\n",
    "    if 'bias' in key:\n",
    "      params += [{'params':[value],'lr':lr*(cfg.TRAIN.DOUBLE_BIAS + 1), \\\n",
    "              'weight_decay': cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY or 0}]\n",
    "    else:\n",
    "      params += [{'params':[value],'lr':lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]\n",
    "\n",
    "if args.optimizer == \"adam\":\n",
    "  lr = lr * 0.1\n",
    "  optimizer = torch.optim.Adam(params)\n",
    "\n",
    "elif args.optimizer == \"sgd\":\n",
    "  optimizer = torch.optim.SGD(params, momentum=cfg.TRAIN.MOMENTUM)\n",
    "\n",
    "if args.cuda:\n",
    "  fasterRCNN.cuda()\n",
    "\n",
    "if args.mGPUs:\n",
    "  fasterRCNN = nn.DataParallel(fasterRCNN)\n",
    "\n",
    "iters_per_epoch = int(train_size / args.batch_size)\n",
    "\n",
    "if args.use_tfboard:\n",
    "  from tensorboardX import SummaryWriter\n",
    "  logger = SummaryWriter(\"logs\")\n",
    "\n",
    "for epoch in range(args.start_epoch, args.max_epochs + 1):\n",
    "  # setting to train mode\n",
    "  fasterRCNN.train()\n",
    "  loss_temp = 0\n",
    "  start = time.time()\n",
    "\n",
    "  if epoch % (args.lr_decay_step + 1) == 0:\n",
    "      adjust_learning_rate(optimizer, args.lr_decay_gamma)\n",
    "      lr *= args.lr_decay_gamma\n",
    "\n",
    "  data_iter = iter(dataloader)\n",
    "  for step in range(iters_per_epoch):\n",
    "    data = next(data_iter)\n",
    "    im_data.data.resize_(data[0].size()).copy_(data[0])\n",
    "    im_info.data.resize_(data[1].size()).copy_(data[1])\n",
    "    gt_boxes.data.resize_(data[2].size()).copy_(data[2])\n",
    "    num_boxes.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "    fasterRCNN.zero_grad()\n",
    "    rois, cls_prob, bbox_pred, \\\n",
    "    rpn_loss_cls, rpn_loss_box, \\\n",
    "    RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "    rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n",
    "\n",
    "    loss = rpn_loss_cls.mean() + rpn_loss_box.mean() \\\n",
    "          + RCNN_loss_cls.mean() + RCNN_loss_bbox.mean()\n",
    "    loss_temp += loss.item()\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if args.net == \"vgg16\":\n",
    "        clip_gradient(fasterRCNN, 10.)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % args.disp_interval == 0:\n",
    "      end = time.time()\n",
    "      if step > 0:\n",
    "        loss_temp /= (args.disp_interval + 1)\n",
    "\n",
    "      if args.mGPUs:\n",
    "        loss_rpn_cls = rpn_loss_cls.mean().item()\n",
    "        loss_rpn_box = rpn_loss_box.mean().item()\n",
    "        loss_rcnn_cls = RCNN_loss_cls.mean().item()\n",
    "        loss_rcnn_box = RCNN_loss_bbox.mean().item()\n",
    "        fg_cnt = torch.sum(rois_label.data.ne(0))\n",
    "        bg_cnt = rois_label.data.numel() - fg_cnt\n",
    "      else:\n",
    "        loss_rpn_cls = rpn_loss_cls.item()\n",
    "        loss_rpn_box = rpn_loss_box.item()\n",
    "        loss_rcnn_cls = RCNN_loss_cls.item()\n",
    "        loss_rcnn_box = RCNN_loss_bbox.item()\n",
    "        fg_cnt = torch.sum(rois_label.data.ne(0))\n",
    "        bg_cnt = rois_label.data.numel() - fg_cnt\n",
    "\n",
    "      print(\"[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e\" \\\n",
    "                              % (args.session, epoch, step, iters_per_epoch, loss_temp, lr))\n",
    "      print(\"\\t\\t\\tfg/bg=(%d/%d), time cost: %f\" % (fg_cnt, bg_cnt, end-start))\n",
    "      print(\"\\t\\t\\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\" \\\n",
    "                    % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_rcnn_box))\n",
    "      if args.use_tfboard:\n",
    "        info = {\n",
    "          'loss': loss_temp,\n",
    "          'loss_rpn_cls': loss_rpn_cls,\n",
    "          'loss_rpn_box': loss_rpn_box,\n",
    "          'loss_rcnn_cls': loss_rcnn_cls,\n",
    "          'loss_rcnn_box': loss_rcnn_box\n",
    "        }\n",
    "        logger.add_scalars(\"logs_s_{}/losses\".format(args.session), info, (epoch - 1) * iters_per_epoch + step)\n",
    "\n",
    "      loss_temp = 0\n",
    "      start = time.time()\n",
    "\n",
    "  \n",
    "  save_name = os.path.join(output_dir, 'faster_rcnn_{}_{}_{}.pth'.format(args.session, epoch, step))\n",
    "  save_checkpoint({\n",
    "    'session': args.session,\n",
    "    'epoch': epoch + 1,\n",
    "    'model': fasterRCNN.module.state_dict() if args.mGPUs else fasterRCNN.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'pooling_mode': cfg.POOLING_MODE,\n",
    "    'class_agnostic': args.class_agnostic,\n",
    "  }, save_name)\n",
    "  print('save model: {}'.format(save_name))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2B_TtCUD84Ys",
    "outputId": "77256c67-597d-45e2-f775-c6246d5df32c"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Tensorflow Faster R-CNN\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n",
    "# Modified by Tao for HW5\n",
    "# --------------------------------------------------------\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "import pdb\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from roi_data_layer.roidb import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.rpn.bbox_transform import clip_boxes\n",
    "#from model.nms.nms_wrapper import nms\n",
    "from model.roi_layers import nms\n",
    "from model.rpn.bbox_transform import bbox_transform_inv\n",
    "from model.utils.net_utils import save_net, load_net, vis_detections\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.faster_rcnn.resnet import resnet\n",
    "\n",
    "\n",
    "try:\n",
    "    xrange  # Python 2\n",
    "except NameError:\n",
    "    xrange = range  # Python 3\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    \"\"\"\n",
    "    Parse input arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n",
    "    parser.add_argument('--dataset', dest='dataset',\n",
    "                        help='training dataset',\n",
    "                        default='uav', type=str)\n",
    "    parser.add_argument('--cfg', dest='cfg_file',\n",
    "                        help='optional config file',\n",
    "                        default='cfgs/vgg16.yml', type=str)\n",
    "    parser.add_argument('--net', dest='net',\n",
    "                        help='vgg16, res50, res101, res152',\n",
    "                        default='res101', type=str)\n",
    "    parser.add_argument('--set', dest='set_cfgs',\n",
    "                        help='set config keys', default=None,\n",
    "                        nargs=argparse.REMAINDER)\n",
    "    parser.add_argument('--save_dir', dest='save_dir',\n",
    "                        help='directory to save models', default=\"models\",\n",
    "                        type=str)\n",
    "    parser.add_argument('--cuda', dest='cuda',\n",
    "                        help='whether use CUDA',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--ls', dest='large_scale',\n",
    "                        help='whether use large imag scale',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--mGPUs', dest='mGPUs',\n",
    "                        help='whether use multiple GPUs',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--cag', dest='class_agnostic',\n",
    "                        help='whether perform class_agnostic bbox regression',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--parallel_type', dest='parallel_type',\n",
    "                        help='which part of model to parallel, 0: all, 1: model before roi pooling',\n",
    "                        default=0, type=int)\n",
    "    parser.add_argument('--checksession', dest='checksession',\n",
    "                        help='checksession to load model',\n",
    "                        default=1, type=int)\n",
    "    parser.add_argument('--checkepoch', dest='checkepoch',\n",
    "                        help='checkepoch to load network',\n",
    "                        default=1, type=int)\n",
    "    parser.add_argument('--checkpoint', dest='checkpoint',\n",
    "                        help='checkpoint to load network',\n",
    "                        default=3960, type=int)\n",
    "    parser.add_argument('--vis', dest='vis',\n",
    "                        help='visualization mode',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--model_dir', dest='model_dir',\n",
    "                        help='directory to save models', default=\"models\",\n",
    "                        type=str)\n",
    "    parser.add_argument('--use_restarting', dest='use_restarting',\n",
    "                        help='where to use restarting',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--is_baseline_method', dest='is_baseline_method',\n",
    "                        help='whether to evaluate the baseline method',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--ovthresh', dest='ovthresh',\n",
    "                        help='the IoU threshold for evaluation',\n",
    "                        default=0.7, type=float)\n",
    "    parser.add_argument('--overall_eval', dest='overall_eval',\n",
    "                        help='display the evaluation results regularly',\n",
    "                        action='store_true')\n",
    "\n",
    "    args = parser.parse_args(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "lr = cfg.TRAIN.LEARNING_RATE\n",
    "momentum = cfg.TRAIN.MOMENTUM\n",
    "weight_decay = cfg.TRAIN.WEIGHT_DECAY\n",
    "\n",
    "\n",
    "\n",
    "args = parse_args(['--dataset','uav','--net','vgg16','--checkepoch','2','--checkpoint','110','--cuda'])\n",
    "\n",
    "args.model_dir = args.model_dir + \"/\" + args.net + \"/\" + args.dataset\n",
    "args.is_baseline_method = True\n",
    "\n",
    "print('Called with args:')\n",
    "print(args)\n",
    "\n",
    "if torch.cuda.is_available() and not args.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "np.random.seed(cfg.RNG_SEED)\n",
    "if args.dataset == \"uav\":\n",
    "    args.imdb_name = \"uav_2017_trainval\"\n",
    "    args.imdbval_name = \"uav_2017_test\"\n",
    "    args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '20']\n",
    "\n",
    "\n",
    "args.cfg_file = \"cfgs/{}_ls.yml\".format(args.net) if args.large_scale else \"cfgs/{}.yml\".format(args.net)\n",
    "\n",
    "if args.cfg_file is not None:\n",
    "    cfg_from_file(args.cfg_file)\n",
    "if args.set_cfgs is not None:\n",
    "    cfg_from_list(args.set_cfgs)\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "\n",
    "cfg.TRAIN.USE_FLIPPED = False\n",
    "imdb, roidb, ratio_list, ratio_index = combined_roidb(args.imdbval_name, False)\n",
    "imdb.competition_mode(on=True)\n",
    "imdb.set_epoch(args.checkepoch)\n",
    "imdb.set_ckpt(args.checkpoint)\n",
    "print('{:d} roidb entries'.format(len(roidb)))\n",
    "\n",
    "nuisance_type =\"Baseline\"\n",
    "\n",
    "model_dir = os.path.join(args.model_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    raise Exception('There is no input directory for loading network from ' + model_dir)\n",
    "\n",
    "load_name = os.path.join(model_dir,'faster_rcnn_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))\n",
    "\n",
    "# initilize the network here.\n",
    "if args.net == 'vgg16':\n",
    "    fasterRCNN = vgg16(imdb.classes, pretrained=False, class_agnostic=args.class_agnostic)\n",
    "elif args.net == 'res101':\n",
    "    fasterRCNN = resnet(imdb.classes, 101, pretrained=False, class_agnostic=args.class_agnostic)\n",
    "elif args.net == 'res50':\n",
    "    fasterRCNN = resnet(imdb.classes, 50, pretrained=False, class_agnostic=args.class_agnostic)\n",
    "elif args.net == 'res152':\n",
    "    fasterRCNN = resnet(imdb.classes, 152, pretrained=False, class_agnostic=args.class_agnostic)\n",
    "else:\n",
    "    print(\"network is not defined\")\n",
    "    pdb.set_trace()\n",
    "\n",
    "fasterRCNN.create_architecture()\n",
    "\n",
    "print(\"load checkpoint %s\" % (load_name))\n",
    "checkpoint = torch.load(load_name)\n",
    "\n",
    "for key in ['RCNN_angle_score.weight', 'RCNN_angle_score.bias',\n",
    "            'RCNN_altitude_score.weight', 'RCNN_altitude_score.bias',\n",
    "            'RCNN_weather_score.weight', 'RCNN_weather_score.bias'\n",
    "            ]:\n",
    "    if key in checkpoint['model'].keys():\n",
    "        del checkpoint['model'][key]\n",
    "\n",
    "model_dict = fasterRCNN.state_dict()\n",
    "model_dict.update(checkpoint['model'])\n",
    "fasterRCNN.load_state_dict(model_dict)\n",
    "\n",
    "# fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "if 'pooling_mode' in checkpoint.keys():\n",
    "    cfg.POOLING_MODE = checkpoint['pooling_mode']\n",
    "\n",
    "print('load model successfully!')\n",
    "# initilize the tensor holder here.\n",
    "im_data = torch.FloatTensor(1)\n",
    "im_info = torch.FloatTensor(1)\n",
    "meta_data = torch.FloatTensor(1)\n",
    "num_boxes = torch.LongTensor(1)\n",
    "gt_boxes = torch.FloatTensor(1)\n",
    "\n",
    "# ship to cuda\n",
    "if args.cuda:\n",
    "    im_data = im_data.cuda()\n",
    "    im_info = im_info.cuda()\n",
    "    meta_data = meta_data.cuda()\n",
    "    num_boxes = num_boxes.cuda()\n",
    "    gt_boxes = gt_boxes.cuda()\n",
    "\n",
    "# make variable\n",
    "im_data = Variable(im_data)\n",
    "im_info = Variable(im_info)\n",
    "meta_data = Variable(meta_data)\n",
    "num_boxes = Variable(num_boxes)\n",
    "gt_boxes = Variable(gt_boxes)\n",
    "\n",
    "if args.cuda:\n",
    "    cfg.CUDA = True\n",
    "\n",
    "if args.cuda:\n",
    "    fasterRCNN.cuda()\n",
    "\n",
    "start = time.time()\n",
    "max_per_image = 100\n",
    "\n",
    "vis = args.vis\n",
    "\n",
    "if vis:\n",
    "    thresh = 0.05\n",
    "else:\n",
    "    thresh = 0.0\n",
    "\n",
    "save_name = 'faster_rcnn_10'\n",
    "num_images = len(imdb.image_index)\n",
    "all_boxes = [[[] for _ in xrange(num_images)]\n",
    "              for _ in xrange(imdb.num_classes)]\n",
    "\n",
    "output_dir = get_output_dir(imdb, save_name)\n",
    "dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n",
    "                          imdb.num_classes, training=False, normalize=False)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                          shuffle=False, num_workers=4,\n",
    "                                          pin_memory=True)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "_t = {'im_detect': time.time(), 'misc': time.time()}\n",
    "det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "fasterRCNN.eval()\n",
    "empty_array = np.transpose(np.array([[], [], [], [], []]), (1, 0))\n",
    "for i in range(num_images):\n",
    "    data = next(data_iter)\n",
    "    im_data.data.resize_(data[0].size()).copy_(data[0])\n",
    "    im_info.data.resize_(data[1].size()).copy_(data[1])\n",
    "    gt_boxes.data.resize_(data[2].size()).copy_(data[2])\n",
    "    num_boxes.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "    det_tic = time.time()\n",
    "    rois, cls_prob, bbox_pred, _, _, _, _, _ = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n",
    "\n",
    "    scores = cls_prob.data\n",
    "    boxes = rois.data[:, :, 1:5]\n",
    "\n",
    "    if cfg.TEST.BBOX_REG:\n",
    "        # Apply bounding-box regression deltas\n",
    "        box_deltas = bbox_pred.data\n",
    "        if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "            # Optionally normalize targets by a precomputed mean and stdev\n",
    "            if args.class_agnostic:\n",
    "                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                              + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                box_deltas = box_deltas.view(1, -1, 4)\n",
    "            else:\n",
    "                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                              + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n",
    "\n",
    "        pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
    "        pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n",
    "    else:\n",
    "        # Simply repeat the boxes, once for each class\n",
    "        pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
    "\n",
    "    pred_boxes /= data[1][0][2].item()\n",
    "\n",
    "    scores = scores.squeeze()\n",
    "    pred_boxes = pred_boxes.squeeze()\n",
    "    det_toc = time.time()\n",
    "    detect_time = det_toc - det_tic\n",
    "    misc_tic = time.time()\n",
    "    if vis:\n",
    "        im = cv2.imread(imdb.image_path_at(i))\n",
    "        im2show = np.copy(im)\n",
    "    for j in xrange(1, imdb.num_classes):\n",
    "        inds = torch.nonzero(scores[:, j] > thresh).view(-1)\n",
    "        # if there is det\n",
    "        if inds.numel() > 0:\n",
    "            cls_scores = scores[:, j][inds]\n",
    "            _, order = torch.sort(cls_scores, 0, True)\n",
    "            if args.class_agnostic:\n",
    "                cls_boxes = pred_boxes[inds, :]\n",
    "            else:\n",
    "                cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n",
    "\n",
    "            cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
    "            # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n",
    "            cls_dets = cls_dets[order]\n",
    "            #keep = nms(cls_dets, cfg.TEST.NMS)\n",
    "            keep = nms(cls_boxes[order, :], cls_scores[order], cfg.TEST.NMS)\n",
    "            cls_dets = cls_dets[keep.view(-1).long()]\n",
    "            if vis:\n",
    "                im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n",
    "            all_boxes[j][i] = cls_dets.cpu().numpy()\n",
    "        else:\n",
    "            all_boxes[j][i] = empty_array\n",
    "\n",
    "    # Limit to max_per_image detections *over all classes*\n",
    "    if max_per_image > 0:\n",
    "        image_scores = np.hstack([all_boxes[j][i][:, -1]\n",
    "                                  for j in xrange(1, imdb.num_classes)])\n",
    "        all_boxes[j][i] = np.concatenate(\n",
    "            (all_boxes[j][i], np.tile(meta_data.cpu().numpy(), (len(image_scores), 1))),\n",
    "            axis=1)\n",
    "        if len(image_scores) > max_per_image:\n",
    "            image_thresh = np.sort(image_scores)[-max_per_image]\n",
    "            for j in xrange(1, imdb.num_classes):\n",
    "                keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n",
    "                all_boxes[j][i] = all_boxes[j][i][keep, :]\n",
    "\n",
    "    misc_toc = time.time()\n",
    "    nms_time = misc_toc - misc_tic\n",
    "\n",
    "    sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n",
    "                      .format(i + 1, num_images, detect_time, nms_time))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if vis:\n",
    "        cv2.imwrite('result.png', im2show)\n",
    "        pdb.set_trace()\n",
    "\n",
    "with open(det_file, 'wb') as f:\n",
    "    pickle.dump(all_boxes, f)\n",
    "\n",
    "print('Evaluating detections')\n",
    "\n",
    "cachedir = 'data/VOCdevkit2007/annotations_cache/data/VOCdevkit2007/UAV2017/ImageSets/Main'\n",
    "if not os.path.isdir(cachedir):\n",
    "    os.makedirs(cachedir)\n",
    "\n",
    "imdb.evaluate_detections(all_boxes, output_dir, nuisance_type=nuisance_type, baseline_method=args.is_baseline_method, ovthresh=args.ovthresh)\n",
    "\n",
    "end = time.time()\n",
    "print(\"test time: %0.4fs\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYFj-a28ZFX9",
    "outputId": "4425ae35-e728-4670-cdfc-13c661729dd1"
   },
   "outputs": [],
   "source": [
    "!python demo.py --net vgg16 \\\n",
    "             --dataset uav\\\n",
    "              --checkepoch 2\\\n",
    "               --checkpoint 110 \\\n",
    "               --cuda --load_dir \"/content/gdrive/MyDrive/nirmal_aayushi_113504530_hw5/faster-rcnn.pytorch/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "I8e_86AfjP_7",
    "outputId": "3fbf3c3d-ea81-44d2-9904-36f4d75a3db2"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#    You need to get detection result images here\n",
    "#--------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the image for local directory (same with this .ipynb) \n",
    "img1 = cv2.imread('images/img0.jpg')\n",
    "img2 = cv2.imread('images/img2.jpg')\n",
    "\n",
    "img1_det = cv2.imread('images/img0_det.jpg')\n",
    "img2_det = cv2.imread('images/img2_det.jpg')\n",
    "\n",
    "# Be sure to convert the color space of the image from\n",
    "# BGR (Opencv) to RGB (Matplotlib) before you show a \n",
    "# color image read from OpenCV\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "plt.title('train image')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(img1_det, cv2.COLOR_BGR2RGB))\n",
    "plt.title('train image (detection)')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "plt.title('test image')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(img2_det, cv2.COLOR_BGR2RGB))\n",
    "plt.title('test image (detection)')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4X3bWAMLhvv"
   },
   "source": [
    "\n",
    "\n",
    "The test results are good since the network is able to detect most of the cars even though the train images are shot at day, the network manages to detect the cars in night shots as well. A lot of cars remain undeteced as well.\n",
    "We are getting some false positives randomly in every run for the test images as the network is falsely detecting some window lights as cars. Such false positives increase as we increase the epochs. \n",
    "\n",
    "We can change hyperparameters like batch size, optimizer and learning rate. As we know by decreasing the learning rate the model performs better. We can also change iou threshold of the bounding boxes for better evaluation of the boxes. We can change the scales and ratios of the anchors. We can also modify the decay for regularization. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "nirmal_aayushi_113504530_hw5_P2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
